{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lzx3T6l_0dglAdc0ajuLQnari3WdB8k0",
      "authorship_tag": "ABX9TyPGBB82fMMMHrKgulN5rABF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bLhmeCEfrpld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "HeF-IEzioRdm",
        "outputId": "871e6d58-53f1-4a8a-fc10-191b3c6f2b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload project file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4bac4339-658e-4e4a-9a1c-e36666f9d56f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4bac4339-658e-4e4a-9a1c-e36666f9d56f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NNIK.zip to NNIK.zip\n",
            "Extracted NNIK.zip\n",
            "Availale Files/Folders\n",
            ".config\n",
            "NNIK.zip\n",
            "NNIK\n",
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "# Import Github repo\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print('Upload project file')\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip, 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')\n",
        "\n",
        "print(f'Extracted {zip}')\n",
        "print('Availale Files/Folders')\n",
        "for item in os.listdir('/content'):\n",
        "  print(f'{item}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to google drive\n",
        "from google.colab import drive\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = Path('/content/NNIK') if Path('/content/NNIK').exists() else Path('/content/NNIK-main')\n",
        "print(f'Project root - {root}')\n",
        "print(f'Project exists - {root.exists()}')\n",
        "\n",
        "remove = [p for p in sys.path if 'NNIK' in p or 'Scripts' in p]\n",
        "for path in remove:\n",
        "  sys.path.remove(path)\n",
        "\n",
        "project_paths = [\n",
        "    str(root),\n",
        "    str(root / 'Scripts'),\n",
        "    str(root / 'Scripts' / 'Models'),\n",
        "    str(root / 'Scripts' / 'Models' / 'Machine_Learning'),\n",
        "    str(root / 'Scripts' / 'Models' / 'Traditional'),\n",
        "]\n",
        "\n",
        "for path in project_paths:\n",
        "  if path not in sys.path:\n",
        "    sys.path.insert(0, path)\n",
        "\n",
        "print('Paths configured')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4HjC1kDqJTK",
        "outputId": "9ed30601-6716-4428-86fb-6aa6d6a9e4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Project root - /content/NNIK\n",
            "Project exists - True\n",
            "Paths configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio scikit-learn pandas matplotlib numpy tqdm pyyaml scipy"
      ],
      "metadata": {
        "id": "F41bZlHHrY2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import project modules\n",
        "import importlib.util\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "\n",
        "def import_path(module_name, file_path):\n",
        "  try:\n",
        "    if not file_path.exists():\n",
        "      print(f'File not found - {file_path}')\n",
        "      return None\n",
        "\n",
        "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
        "    if spec is None or spec.loader is None:\n",
        "      print(f'Couldnt create spec for {module_name}')\n",
        "      return None\n",
        "\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[module_name] = module\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n",
        "  except Exception as e:\n",
        "    print(f'Error importing {module_name} - {str(e)}')\n",
        "    return None\n",
        "\n",
        "utils_module = import_path(\"utils\", root / \"Scripts\" / \"utils.py\")\n",
        "data_gen_module = import_path(\"data_gen\", root / \"Scripts\" / \"data_gen.py\")\n",
        "training_module = import_path(\"training\", root / \"Scripts\" / \"training.py\")\n",
        "testing_module = import_path(\"testing\", root / \"Scripts\" / \"testing.py\")\n",
        "\n",
        "if utils_module:\n",
        "    print(\"utils.py imported\")\n",
        "if data_gen_module:\n",
        "    print(\"data_gen.py imported\")\n",
        "if training_module:\n",
        "    print(\"training.py imported\")\n",
        "if testing_module:\n",
        "    print(\"testing.py imported\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qG_IXXMrm2l",
        "outputId": "51d4a5d3-6a46-4cac-8294-c9f065a7d5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utils.py imported\n",
            "data_gen.py imported\n",
            "training.py imported\n",
            "testing.py imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test (hope it dont break) -> it works!\n",
        "data_directory = root / 'data'\n",
        "print(f'data directory - {data_directory}')\n",
        "print(f'data directory exists - {data_directory.exists()}')\n",
        "\n",
        "if data_directory.exists():\n",
        "  print('\\nTraining data - ')\n",
        "  training_directory = data_directory / 'Training'\n",
        "  if training_directory.exists():\n",
        "    training_files = list(training_directory.glob('*.json'))\n",
        "    print(f'Found {len(training_files)} training files')\n",
        "    for f in sorted(training_files)[:5]:\n",
        "      print(f.name)\n",
        "  print('\\nTesting data:')\n",
        "  testing_directory = data_directory / 'Testing'\n",
        "  if testing_directory.exists():\n",
        "    testing_files = list(testing_directory.glob('*.json'))\n",
        "    print(f'Found {len(testing_files)} testing files')\n",
        "    for f in sorted(testing_files)[:5]:\n",
        "      print(f.name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CZyl_ljuig8",
        "outputId": "8b5f1336-e724-415f-e550-4af21af59a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data directory - /content/NNIK/data\n",
            "data directory exists - True\n",
            "\n",
            "Training data - \n",
            "Found 16 training files\n",
            "10_training.json\n",
            "10_training_solutions.json\n",
            "3_training.json\n",
            "3_training_solutions.json\n",
            "4_training.json\n",
            "\n",
            "Testing data:\n",
            "Found 16 testing files\n",
            "10_testing.json\n",
            "10_testing_solutions.json\n",
            "3_testing.json\n",
            "3_testing_solutions.json\n",
            "4_testing.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Scripts.training import load_ik_data, train_all_models, evaluate_all_models, create_results_dataframe\n",
        "from Scripts.testing import create_models\n",
        "import time\n",
        "import torch\n",
        "\n",
        "dof_range = [3, 4, 5, 6]\n",
        "sample_limit = 1000\n",
        "\n",
        "trained_models = {}\n",
        "summary = []\n",
        "\n",
        "for dof in dof_range:\n",
        "    print(f\"\\nDOF={dof} training\")\n",
        "\n",
        "    train_poses = data_directory / 'Training' / f'{dof}_training.json'\n",
        "    train_solutions = data_directory / 'Training' / f'{dof}_training_solutions.json'\n",
        "\n",
        "    if not (train_poses.exists() and train_solutions.exists()):\n",
        "        print(f\"no data for DOF={dof}\")\n",
        "        continue\n",
        "\n",
        "    X_train, y_train = load_ik_data(train_poses, train_solutions)\n",
        "\n",
        "    if sample_limit and len(X_train) > sample_limit:\n",
        "        idx = np.random.choice(len(X_train), sample_limit, replace=False)\n",
        "        X_train, y_train = X_train[idx], y_train[idx]\n",
        "\n",
        "    print(f\"Training data - {X_train.shape} -> {y_train.shape}\")\n",
        "\n",
        "    models_for_dof = create_models(input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
        "\n",
        "    selected_models = {}\n",
        "    for name in ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'GPR', 'MDN', 'CVAE']:\n",
        "        if name in models_for_dof and models_for_dof[name] is not None:\n",
        "            selected_models[name] = models_for_dof[name]\n",
        "\n",
        "    print(f\"models: {list(selected_models.keys())}\")\n",
        "\n",
        "    trained_models_dof = {}\n",
        "\n",
        "    for model_name, model in selected_models.items():\n",
        "        print(f\"\\nTraining {model_name}\")\n",
        "        print(f\"GPU?: {torch.cuda.is_available()}\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            trained_models_dof[model_name] = {\n",
        "                'model': model,\n",
        "                'training_time': training_time,\n",
        "                'dof': dof\n",
        "            }\n",
        "\n",
        "            print(f\"{model_name} trained - {training_time:.2f}s\")\n",
        "\n",
        "            summary.append({\n",
        "                'dof': dof,\n",
        "                'model': model_name,\n",
        "                'training_time': training_time,\n",
        "                'samples': len(X_train),\n",
        "                'status': 'success'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name} failed - {str(e)}\")\n",
        "            summary.append({\n",
        "                'dof': dof,\n",
        "                'model': model_name,\n",
        "                'training_time': 0,\n",
        "                'samples': len(X_train),\n",
        "                'status': f'failed: {str(e)[:50]}'\n",
        "            })\n",
        "\n",
        "    trained_models[dof] = trained_models_dof\n",
        "\n",
        "    print(f\"\\nDOF={dof} training complete - {len(trained_models_dof)} models trained\")\n",
        "\n",
        "print(f\"\\nTraining Complete\")\n",
        "\n",
        "training_df = pd.DataFrame(summary)\n",
        "if not training_df.empty:\n",
        "    print(\"Training Summary:\")\n",
        "    print(training_df)\n",
        "\n",
        "    successful = training_df[training_df['status'] == 'success']\n",
        "    if not successful.empty:\n",
        "        print(f\"\\nStatistics:\")\n",
        "        pivot = successful.pivot_table(values='training_time', index='model', columns='dof', aggfunc='mean')\n",
        "        print(pivot.round(2))\n",
        "\n",
        "print(f\"\\nTotal trained models: {sum(len(models) for models in trained_models.values())}\")"
      ],
      "metadata": {
        "id": "KsDC2C0kS1FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "all_test_results = []\n",
        "testing_summary = []\n",
        "\n",
        "for dof, trained_models_dof in trained_models.items():\n",
        "    if not trained_models_dof:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nDOF={dof} testing\")\n",
        "\n",
        "    test_poses = data_directory / 'Testing' / f'{dof}_testing.json'\n",
        "    test_solutions = data_directory / 'Testing' / f'{dof}_testing_solutions.json'\n",
        "\n",
        "    if not (test_poses.exists() and test_solutions.exists()):\n",
        "        print(f\"testing data not found for dof={dof}\")\n",
        "        continue\n",
        "\n",
        "    X_test, y_test = load_ik_data(test_poses, test_solutions)\n",
        "\n",
        "    if sample_limit and len(X_test) > sample_limit//2:\n",
        "        idx = np.random.choice(len(X_test), sample_limit//2, replace=False)\n",
        "        X_test, y_test = X_test[idx], y_test[idx]\n",
        "\n",
        "    print(f\"Testing data: {X_test.shape} -> {y_test.shape}\")\n",
        "\n",
        "    for model_name, model_data in trained_models_dof.items():\n",
        "        print(f\"\\nTesting {model_name}\")\n",
        "\n",
        "        try:\n",
        "            model = model_data['model']\n",
        "            start_time = time.time()\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            joint_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "            position_rmse = joint_rmse\n",
        "\n",
        "            test_result = {\n",
        "                'dof': dof,\n",
        "                'model': model_name,\n",
        "                'position_rmse': position_rmse,\n",
        "                'joint_rmse': joint_rmse,\n",
        "                'training_time': model_data['training_time'],\n",
        "                'inference_time': inference_time,\n",
        "                'inference_time_per_sample': inference_time / len(X_test),\n",
        "                'test_samples': len(X_test),\n",
        "                'status': 'success'\n",
        "            }\n",
        "\n",
        "            all_test_results.append(test_result)\n",
        "\n",
        "            print(f\"{model_name}: Joint RMSE = {joint_rmse:.4f}\")\n",
        "            print(f\"Inference: {inference_time:.3f}s ({inference_time/len(X_test)*1000:.2f}ms/sample)\")\n",
        "\n",
        "            testing_summary.append({\n",
        "                'dof': dof,\n",
        "                'model': model_name,\n",
        "                'joint_rmse': joint_rmse,\n",
        "                'inference_time': inference_time,\n",
        "                'status': 'success'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name} testing failed: {str(e)}\")\n",
        "            testing_summary.append({\n",
        "                'dof': dof,\n",
        "                'model': model_name,\n",
        "                'joint_rmse': float('inf'),\n",
        "                'inference_time': 0,\n",
        "                'status': f'failed: {str(e)[:50]}'\n",
        "            })\n",
        "\n",
        "    print(f\"\\ndof={dof} testing complete\")\n",
        "\n",
        "print(f\"\\ntesting complete\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if all_test_results:\n",
        "    results_df = pd.DataFrame(all_test_results)\n",
        "\n",
        "    print(\"Complete Results:\")\n",
        "    display_cols = ['dof', 'model', 'joint_rmse', 'training_time', 'inference_time_per_sample']\n",
        "    print(results_df[display_cols].round(4))\n",
        "\n",
        "    results = root / 'results'\n",
        "    results.mkdir(exist_ok=True)\n",
        "    results_df.to_csv(results / 'explicit_training_results.csv', index=False)\n",
        "    print(f\"\\nResults saved to: {results / 'explicit_training_results.csv'}\")\n",
        "\n",
        "    print(f\"\\nModel Performance Summary (Average across DOFs):\")\n",
        "    model_summary = results_df.groupby('model').agg({\n",
        "        'joint_rmse': 'mean',\n",
        "        'training_time': 'mean',\n",
        "        'inference_time_per_sample': 'mean'\n",
        "    }).round(4)\n",
        "    print(model_summary)\n",
        "\n",
        "else:\n",
        "    print(\"No test results available\")"
      ],
      "metadata": {
        "id": "1DqJqBy7VMd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
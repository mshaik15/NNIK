{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bLhmeCEfrpld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "HeF-IEzioRdm",
        "outputId": "36fa798d-9116-4603-fde9-6e1333cd9f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload project file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a52b4dab-aef3-4d55-a3fb-7b2ae5c80e34\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a52b4dab-aef3-4d55-a3fb-7b2ae5c80e34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NNIK.zip to NNIK.zip\n",
            "Extracted NNIK.zip\n",
            "Availale Files/Folders\n",
            ".config\n",
            "NNIK.zip\n",
            "NNIK\n",
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "# Import Github repo\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print('Upload project file')\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip, 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')\n",
        "\n",
        "print(f'Extracted {zip}')\n",
        "print('Availale Files/Folders')\n",
        "for item in os.listdir('/content'):\n",
        "  print(f'{item}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSgsNZ1SxuPe",
        "outputId": "2823fac0-1a52-45db-a7a1-07d18af4997d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Paths\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path('/content/NNIK') if Path('/content/NNIK').exists() else Path('/content/NNIK-main')\n",
        "print(f'Project root - {root}')\n",
        "print(f'Project exists - {root.exists()}')\n",
        "\n",
        "remove = [p for p in sys.path if 'NNIK' in p or 'Scripts' in p]\n",
        "for path in remove:\n",
        "  sys.path.remove(path)\n",
        "\n",
        "project_paths = [\n",
        "    str(root),\n",
        "    str(root / 'Scripts'),\n",
        "    str(root / 'Scripts' / 'Models'),\n",
        "    str(root / 'Scripts' / 'Models' / 'Machine_Learning'),\n",
        "    str(root / 'Scripts' / 'Models' / 'Traditional'),\n",
        "]\n",
        "\n",
        "for path in project_paths:\n",
        "  if path not in sys.path:\n",
        "    sys.path.insert(0, path)\n",
        "\n",
        "print('Paths configured')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4HjC1kDqJTK",
        "outputId": "9d2a1d23-042a-4fba-f657-fe2b9868d722"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project root - /content/NNIK\n",
            "Project exists - True\n",
            "Paths configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio scikit-learn pandas matplotlib numpy tqdm pyyaml scipy"
      ],
      "metadata": {
        "id": "F41bZlHHrY2d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import project modules\n",
        "import importlib.util\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "\n",
        "def import_path(module_name, file_path):\n",
        "  try:\n",
        "    if not file_path.exists():\n",
        "      print(f'File not found - {file_path}')\n",
        "      return None\n",
        "\n",
        "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
        "    if spec is None or spec.loader is None:\n",
        "      print(f'Couldnt create spec for {module_name}')\n",
        "      return None\n",
        "\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[module_name] = module\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n",
        "  except Exception as e:\n",
        "    print(f'Error importing {module_name} - {str(e)}')\n",
        "    return None\n",
        "\n",
        "utils_module = import_path('utils', root / 'Scripts' / 'utils.py')\n",
        "data_gen_module = import_path('data_gen', root / 'Scripts' / 'data_gen.py')\n",
        "training_module = import_path('training', root / 'Scripts' / 'training.py')\n",
        "testing_module = import_path('testing', root / 'Scripts' / 'testing.py')\n",
        "\n",
        "if utils_module:\n",
        "    print('utils.py imported')\n",
        "if data_gen_module:\n",
        "    print('data_gen.py imported')\n",
        "if training_module:\n",
        "    print('training.py imported')\n",
        "if testing_module:\n",
        "    print('testing.py imported')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qG_IXXMrm2l",
        "outputId": "92056e31-fdd7-4bf7-986e-a6be3fca6fba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utils.py imported\n",
            "data_gen.py imported\n",
            "training.py imported\n",
            "testing.py imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test\n",
        "data_directory = root / 'data'\n",
        "print(f'data directory - {data_directory}')\n",
        "print(f'data directory exists - {data_directory.exists()}')\n",
        "\n",
        "if data_directory.exists():\n",
        "  print('\\nTraining data - ')\n",
        "  training_directory = data_directory / 'Training'\n",
        "  if training_directory.exists():\n",
        "    training_files = list(training_directory.glob('*.json'))\n",
        "    print(f'Found {len(training_files)} training files')\n",
        "    for f in sorted(training_files)[:5]:\n",
        "      print(f.name)\n",
        "  print('\\nTesting data:')\n",
        "  testing_directory = data_directory / 'Testing'\n",
        "  if testing_directory.exists():\n",
        "    testing_files = list(testing_directory.glob('*.json'))\n",
        "    print(f'Found {len(testing_files)} testing files')\n",
        "    for f in sorted(testing_files)[:5]:\n",
        "      print(f.name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CZyl_ljuig8",
        "outputId": "9aa62326-a1f9-4e5e-8992-dd5ef5a3d296"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data directory - /content/NNIK/data\n",
            "data directory exists - True\n",
            "\n",
            "Training data - \n",
            "Found 16 training files\n",
            "10_training.json\n",
            "10_training_solutions.json\n",
            "3_training.json\n",
            "3_training_solutions.json\n",
            "4_training.json\n",
            "\n",
            "Testing data:\n",
            "Found 16 testing files\n",
            "10_testing.json\n",
            "10_testing_solutions.json\n",
            "3_testing.json\n",
            "3_testing_solutions.json\n",
            "4_testing.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "from Scripts.training import load_ik_data, train_all_models, setup_gpu\n",
        "from Scripts.testing import create_models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "dof_range = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "sample_limit = None\n",
        "\n",
        "device = setup_gpu()\n",
        "\n",
        "\n",
        "all_trained_models = {}\n",
        "all_training_results = []\n",
        "\n",
        "for dof in dof_range:\n",
        "    print(f'\\nDOF={dof} training')\n",
        "\n",
        "    train_poses = data_directory / 'Training' / f'{dof}_training.json'\n",
        "    train_solutions = data_directory / 'Training' / f'{dof}_training_solutions.json'\n",
        "\n",
        "    if not (train_poses.exists() and train_solutions.exists()):\n",
        "        print(f'no data for DOF={dof}')\n",
        "        continue\n",
        "\n",
        "    X_train, y_train = load_ik_data(train_poses, train_solutions)\n",
        "\n",
        "    if sample_limit and len(X_train) > sample_limit:\n",
        "        idx = np.random.choice(len(X_train), sample_limit, replace=False)\n",
        "        X_train, y_train = X_train[idx], y_train[idx]\n",
        "\n",
        "    print(f'Training data - {X_train.shape} -> {y_train.shape}')\n",
        "\n",
        "    models_for_dof = create_models(input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
        "\n",
        "    selected_models = {}\n",
        "    # Remove GPR for now - it's too slow with 2000 samples\n",
        "    for name in ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']:  # Removed GPR\n",
        "        if name in models_for_dof and models_for_dof[name] is not None:\n",
        "            selected_models[name] = models_for_dof[name]\n",
        "\n",
        "    print(f'models: {list(selected_models.keys())}')\n",
        "\n",
        "    trained_models = train_all_models(selected_models, X_train, y_train, use_parallel=True)\n",
        "\n",
        "    all_trained_models[dof] = trained_models\n",
        "\n",
        "    for name, result in trained_models.items():\n",
        "        if 'error' not in result:\n",
        "            all_training_results.append({\n",
        "                'dof': dof,\n",
        "                'model': name,\n",
        "                'training_time': result['training_time'],\n",
        "                'samples': len(X_train),\n",
        "                'status': 'success'\n",
        "            })\n",
        "        else:\n",
        "            all_training_results.append({\n",
        "                'dof': dof,\n",
        "                'model': name,\n",
        "                'training_time': 0,\n",
        "                'samples': len(X_train),\n",
        "                'status': f'failed: {result['error'][:50]}'\n",
        "            })\n",
        "\n",
        "    print(f'\\nDOF={dof} training complete - {len([r for r in trained_models.values() if 'error' not in r])} models trained')\n",
        "\n",
        "print(f'\\nTraining Complete')\n",
        "\n",
        "training_df = pd.DataFrame(all_training_results)\n",
        "if not training_df.empty:\n",
        "    print('Training Summary:')\n",
        "    print(training_df)\n",
        "\n",
        "    successful = training_df[training_df['status'] == 'success']\n",
        "    if not successful.empty:\n",
        "        print(f'\\nStatistics:')\n",
        "        pivot = successful.pivot_table(values='training_time', index='model', columns='dof', aggfunc='mean')\n",
        "        print(pivot.round(2))\n",
        "\n",
        "print(f'\\nTotal trained models: {sum(len([m for m in models.values() if 'error' not in m]) for models in all_trained_models.values())}')"
      ],
      "metadata": {
        "id": "KsDC2C0kS1FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7178259d-765b-489c-9823-32a4699d9915"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "DOF=3 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 3)\n",
            "DOF from data: 3\n",
            "Training data - (2000, 6) -> (2000, 3)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ KNN completed in 0.02s\n",
            "  ✓ ELM completed in 0.25s\n",
            "  ✓ RandomForest completed in 0.50s\n",
            "  ✓ SVM completed in 0.71s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.5440, Val Loss: 0.4884\n",
            "  ✓ ANN completed in 7.04s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: -0.4401\n",
            "Epoch [40/50], Loss: -1.6126\n",
            "  ✓ MDN completed in 14.99s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 0.9036\n",
            "Epoch [40/50], Loss: 0.5735\n",
            "  ✓ CVAE completed in 9.81s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=3 training complete - 7 models trained\n",
            "\n",
            "DOF=4 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 4)\n",
            "DOF from data: 4\n",
            "Training data - (2000, 6) -> (2000, 4)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ KNN completed in 0.01s\n",
            "  ✓ ELM completed in 0.03s\n",
            "  ✓ RandomForest completed in 0.59s\n",
            "  ✓ SVM completed in 0.95s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.6993, Val Loss: 0.6986\n",
            "  ✓ ANN completed in 0.82s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 1.7754\n",
            "Epoch [40/50], Loss: 0.9933\n",
            "  ✓ MDN completed in 9.88s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 2.1093\n",
            "Epoch [40/50], Loss: 1.5820\n",
            "  ✓ CVAE completed in 10.83s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=4 training complete - 7 models trained\n",
            "\n",
            "DOF=5 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 5)\n",
            "DOF from data: 5\n",
            "Training data - (2000, 6) -> (2000, 5)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ KNN completed in 0.01s\n",
            "  ✓ ELM completed in 0.10s\n",
            "  ✓ RandomForest completed in 0.55s\n",
            "  ✓ SVM completed in 1.22s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.8068, Val Loss: 0.8292\n",
            "  ✓ ANN completed in 0.88s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 5.2371\n",
            "Epoch [40/50], Loss: 4.8836\n",
            "  ✓ MDN completed in 10.05s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 3.3176\n",
            "Epoch [40/50], Loss: 3.0256\n",
            "  ✓ CVAE completed in 9.92s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=5 training complete - 7 models trained\n",
            "\n",
            "DOF=6 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 6)\n",
            "DOF from data: 6\n",
            "Training data - (2000, 6) -> (2000, 6)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "  ✓ KNN completed in 0.01s\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ ELM completed in 0.06s\n",
            "  ✓ RandomForest completed in 0.71s\n",
            "  ✓ SVM completed in 1.56s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.9115, Val Loss: 0.9211\n",
            "  ✓ ANN completed in 0.84s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 7.2759\n",
            "Epoch [40/50], Loss: 7.0293\n",
            "  ✓ MDN completed in 10.60s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 4.6022\n",
            "Epoch [40/50], Loss: 4.3730\n",
            "  ✓ CVAE completed in 9.84s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=6 training complete - 7 models trained\n",
            "\n",
            "DOF=7 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 7)\n",
            "DOF from data: 7\n",
            "Training data - (2000, 6) -> (2000, 7)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ KNN completed in 0.01s\n",
            "  ✓ ELM completed in 0.01s\n",
            "  ✓ RandomForest completed in 0.60s\n",
            "  ✓ SVM completed in 2.23s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.9292, Val Loss: 0.9655\n",
            "  ✓ ANN completed in 1.14s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 9.0014\n",
            "Epoch [40/50], Loss: 8.7922\n",
            "  ✓ MDN completed in 9.67s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 5.6083\n",
            "Epoch [40/50], Loss: 5.4821\n",
            "  ✓ CVAE completed in 10.05s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=7 training complete - 7 models trained\n",
            "\n",
            "DOF=8 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 8)\n",
            "DOF from data: 8\n",
            "Training data - (2000, 6) -> (2000, 8)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ KNN completed in 0.01s\n",
            "  ✓ ELM completed in 0.03s\n",
            "  ✓ RandomForest completed in 0.66s\n",
            "  ✓ SVM completed in 1.78s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.9540, Val Loss: 0.9888\n",
            "  ✓ ANN completed in 0.84s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 10.5752\n",
            "Epoch [40/50], Loss: 10.3110\n",
            "  ✓ MDN completed in 10.04s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 6.4972\n",
            "Epoch [40/50], Loss: 6.3695\n",
            "  ✓ CVAE completed in 9.97s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=8 training complete - 7 models trained\n",
            "\n",
            "DOF=9 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 9)\n",
            "DOF from data: 9\n",
            "Training data - (2000, 6) -> (2000, 9)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "Training ELM...\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ KNN completed in 0.03s\n",
            "  ✓ ELM completed in 0.05s\n",
            "  ✓ RandomForest completed in 0.69s\n",
            "  ✓ SVM completed in 1.97s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.9713, Val Loss: 0.9614\n",
            "  ✓ ANN completed in 0.88s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 11.9833\n",
            "Epoch [40/50], Loss: 11.7154\n",
            "  ✓ MDN completed in 10.16s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 7.4682\n",
            "Epoch [40/50], Loss: 7.1682\n",
            "  ✓ CVAE completed in 9.38s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=9 training complete - 7 models trained\n",
            "\n",
            "DOF=10 training\n",
            "Loaded data: X shape = (2000, 6), y shape = (2000, 10)\n",
            "DOF from data: 10\n",
            "Training data - (2000, 6) -> (2000, 10)\n",
            "models: ['ANN', 'KNN', 'ELM', 'RandomForest', 'SVM', 'MDN', 'CVAE']\n",
            "GPU: Tesla T4 (15.8 GB)\n",
            "\n",
            "Training 4 CPU models in parallel...\n",
            "Training KNN...\n",
            "  ✓ KNN completed in 0.01s\n",
            "Training ELM...\n",
            "  ✓ ELM completed in 0.02s\n",
            "Training RandomForest...\n",
            "Training SVM...\n",
            "  ✓ RandomForest completed in 1.01s\n",
            "  ✓ SVM completed in 3.05s\n",
            "\n",
            "Training 3 GPU models...\n",
            "Training ANN...\n",
            "  Using GPU for ANN\n",
            "Epoch [20/20], Train Loss: 0.9901, Val Loss: 1.0063\n",
            "  ✓ ANN completed in 0.84s\n",
            "Training MDN...\n",
            "  Using GPU for MDN\n",
            "Epoch [20/50], Loss: 13.5454\n",
            "Epoch [40/50], Loss: 13.2937\n",
            "  ✓ MDN completed in 9.71s\n",
            "Training CVAE...\n",
            "  Using GPU for CVAE\n",
            "Epoch [20/50], Loss: 8.5286\n",
            "Epoch [40/50], Loss: 8.2364\n",
            "  ✓ CVAE completed in 9.68s\n",
            "\n",
            "GPU Memory: 0.02 GB allocated\n",
            "\n",
            "DOF=10 training complete - 7 models trained\n",
            "\n",
            "Training Complete\n",
            "Training Summary:\n",
            "    dof         model  training_time  samples   status\n",
            "0     3           KNN       0.018075     2000  success\n",
            "1     3           ELM       0.245470     2000  success\n",
            "2     3  RandomForest       0.500202     2000  success\n",
            "3     3           SVM       0.706417     2000  success\n",
            "4     3           ANN       7.041619     2000  success\n",
            "5     3           MDN      14.986474     2000  success\n",
            "6     3          CVAE       9.813835     2000  success\n",
            "7     4           KNN       0.007976     2000  success\n",
            "8     4           ELM       0.029209     2000  success\n",
            "9     4  RandomForest       0.592991     2000  success\n",
            "10    4           SVM       0.947817     2000  success\n",
            "11    4           ANN       0.817864     2000  success\n",
            "12    4           MDN       9.878976     2000  success\n",
            "13    4          CVAE      10.829699     2000  success\n",
            "14    5           KNN       0.009183     2000  success\n",
            "15    5           ELM       0.097826     2000  success\n",
            "16    5  RandomForest       0.550972     2000  success\n",
            "17    5           SVM       1.224242     2000  success\n",
            "18    5           ANN       0.880101     2000  success\n",
            "19    5           MDN      10.054645     2000  success\n",
            "20    5          CVAE       9.916422     2000  success\n",
            "21    6           KNN       0.008019     2000  success\n",
            "22    6           ELM       0.063813     2000  success\n",
            "23    6  RandomForest       0.706127     2000  success\n",
            "24    6           SVM       1.560040     2000  success\n",
            "25    6           ANN       0.837391     2000  success\n",
            "26    6           MDN      10.597624     2000  success\n",
            "27    6          CVAE       9.836516     2000  success\n",
            "28    7           KNN       0.007688     2000  success\n",
            "29    7           ELM       0.014790     2000  success\n",
            "30    7  RandomForest       0.602273     2000  success\n",
            "31    7           SVM       2.227622     2000  success\n",
            "32    7           ANN       1.142401     2000  success\n",
            "33    7           MDN       9.672147     2000  success\n",
            "34    7          CVAE      10.045665     2000  success\n",
            "35    8           KNN       0.007810     2000  success\n",
            "36    8           ELM       0.028353     2000  success\n",
            "37    8  RandomForest       0.655192     2000  success\n",
            "38    8           SVM       1.784944     2000  success\n",
            "39    8           ANN       0.841382     2000  success\n",
            "40    8           MDN      10.035006     2000  success\n",
            "41    8          CVAE       9.967128     2000  success\n",
            "42    9           KNN       0.034721     2000  success\n",
            "43    9           ELM       0.054039     2000  success\n",
            "44    9  RandomForest       0.692192     2000  success\n",
            "45    9           SVM       1.966713     2000  success\n",
            "46    9           ANN       0.877033     2000  success\n",
            "47    9           MDN      10.163261     2000  success\n",
            "48    9          CVAE       9.381102     2000  success\n",
            "49   10           KNN       0.006258     2000  success\n",
            "50   10           ELM       0.019372     2000  success\n",
            "51   10  RandomForest       1.006132     2000  success\n",
            "52   10           SVM       3.049470     2000  success\n",
            "53   10           ANN       0.844563     2000  success\n",
            "54   10           MDN       9.706816     2000  success\n",
            "55   10          CVAE       9.675563     2000  success\n",
            "\n",
            "Statistics:\n",
            "dof              3      4      5      6      7      8      9     10\n",
            "model                                                              \n",
            "ANN            7.04   0.82   0.88   0.84   1.14   0.84   0.88  0.84\n",
            "CVAE           9.81  10.83   9.92   9.84  10.05   9.97   9.38  9.68\n",
            "ELM            0.25   0.03   0.10   0.06   0.01   0.03   0.05  0.02\n",
            "KNN            0.02   0.01   0.01   0.01   0.01   0.01   0.03  0.01\n",
            "MDN           14.99   9.88  10.05  10.60   9.67  10.04  10.16  9.71\n",
            "RandomForest   0.50   0.59   0.55   0.71   0.60   0.66   0.69  1.01\n",
            "SVM            0.71   0.95   1.22   1.56   2.23   1.78   1.97  3.05\n",
            "\n",
            "Total trained models: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "from Scripts.testing import evaluate_all_models, create_results_dataframe\n",
        "\n",
        "all_test_results = []\n",
        "\n",
        "print('\\n')\n",
        "print('Testing')\n",
        "\n",
        "for dof, trained_models in all_trained_models.items():\n",
        "    if not trained_models:\n",
        "        continue\n",
        "\n",
        "    print(f'\\nDOF={dof} testing')\n",
        "\n",
        "    test_poses = data_directory / 'Testing' / f'{dof}_testing.json'\n",
        "    test_solutions = data_directory / 'Testing' / f'{dof}_testing_solutions.json'\n",
        "\n",
        "    if not (test_poses.exists() and test_solutions.exists()):\n",
        "        print(f'testing data not found for dof={dof}')\n",
        "        continue\n",
        "\n",
        "    X_test, y_test = load_ik_data(test_poses, test_solutions)\n",
        "\n",
        "    if sample_limit and len(X_test) > sample_limit//2:\n",
        "        idx = np.random.choice(len(X_test), sample_limit//2, replace=False)\n",
        "        X_test, y_test = X_test[idx], y_test[idx]\n",
        "\n",
        "    print(f'Testing data: {X_test.shape} -> {y_test.shape}')\n",
        "\n",
        "    evaluation_results = evaluate_all_models(trained_models, X_test, y_test, force_cpu=False)\n",
        "\n",
        "    for name, results in evaluation_results.items():\n",
        "        if 'error' not in results:\n",
        "            all_test_results.append({\n",
        "                'dof': dof,\n",
        "                'model': name,\n",
        "                'position_rmse': results['position_rmse'],\n",
        "                'joint_rmse': results['joint_rmse'],\n",
        "                'training_time': results['training_time'],\n",
        "                'inference_time': results['inference_time'],\n",
        "                'inference_time_per_sample': results['inference_time_per_sample'],\n",
        "                'test_samples': len(X_test),\n",
        "                'status': 'success'\n",
        "            })\n",
        "\n",
        "    print(f'\\ndof={dof} testing complete')\n",
        "\n",
        "print(f'\\ntesting complete')\n",
        "print('='*60)\n",
        "\n",
        "if all_test_results:\n",
        "    results_df = pd.DataFrame(all_test_results)\n",
        "\n",
        "    print('Complete Results:')\n",
        "    display_cols = ['dof', 'model', 'joint_rmse', 'training_time', 'inference_time_per_sample']\n",
        "    print(results_df[display_cols].round(4))\n",
        "\n",
        "    results_path = root / 'results'\n",
        "    results_path.mkdir(exist_ok=True)\n",
        "    results_df.to_csv(results_path / 'explicit_training_results.csv', index=False)\n",
        "    print(f'\\nResults saved to: {results_path / 'explicit_training_results.csv'}')\n",
        "\n",
        "    print(f'\\nModel Performance Summary (Average across DOFs):')\n",
        "    model_summary = results_df.groupby('model').agg({\n",
        "        'joint_rmse': 'mean',\n",
        "        'training_time': 'mean',\n",
        "        'inference_time_per_sample': 'mean'\n",
        "    }).round(4)\n",
        "    print(model_summary)\n",
        "\n",
        "else:\n",
        "    print('No test results available')"
      ],
      "metadata": {
        "id": "1DqJqBy7VMd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2b2ab9-ce29-4ce1-e92d-0f65d4afc128"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Testing\n",
            "\n",
            "DOF=3 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 3)\n",
            "DOF from data: 3\n",
            "Testing data: (1000, 6) -> (1000, 3)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.0648, Inference: 0.008s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.3555, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 0.8570, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.1026, Inference: 0.127s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.3034, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.3428, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 0.9618, Inference: 0.005s\n",
            "\n",
            "dof=3 testing complete\n",
            "\n",
            "DOF=4 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 4)\n",
            "DOF from data: 4\n",
            "Testing data: (1000, 6) -> (1000, 4)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.4249, Inference: 0.010s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.5366, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.3798, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.4019, Inference: 0.266s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.4887, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.5188, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 1.3593, Inference: 0.001s\n",
            "\n",
            "dof=4 testing complete\n",
            "\n",
            "DOF=5 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 5)\n",
            "DOF from data: 5\n",
            "Testing data: (1000, 6) -> (1000, 5)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.6308, Inference: 0.014s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.6597, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.5741, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.5898, Inference: 0.500s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.6392, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.6246, Inference: 0.008s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 1.8070, Inference: 0.001s\n",
            "\n",
            "dof=5 testing complete\n",
            "\n",
            "DOF=6 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 6)\n",
            "DOF from data: 6\n",
            "Testing data: (1000, 6) -> (1000, 6)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.8155, Inference: 0.022s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.7616, Inference: 0.001s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.7764, Inference: 0.029s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.7739, Inference: 1.068s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.7471, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.7507, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 2.0519, Inference: 0.001s\n",
            "\n",
            "dof=6 testing complete\n",
            "\n",
            "DOF=7 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 7)\n",
            "DOF from data: 7\n",
            "Testing data: (1000, 6) -> (1000, 7)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.9012, Inference: 0.013s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.8015, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.8182, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.8296, Inference: 0.552s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.7809, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.7861, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 2.1149, Inference: 0.001s\n",
            "\n",
            "dof=7 testing complete\n",
            "\n",
            "DOF=8 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 8)\n",
            "DOF from data: 8\n",
            "Testing data: (1000, 6) -> (1000, 8)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.9224, Inference: 0.015s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.8035, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.8431, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.8458, Inference: 0.654s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.7904, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.8047, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 2.1735, Inference: 0.001s\n",
            "\n",
            "dof=8 testing complete\n",
            "\n",
            "DOF=9 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 9)\n",
            "DOF from data: 9\n",
            "Testing data: (1000, 6) -> (1000, 9)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.9272, Inference: 0.014s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.7976, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.8416, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.8559, Inference: 0.752s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.7931, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.7973, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 2.1482, Inference: 0.001s\n",
            "\n",
            "dof=9 testing complete\n",
            "\n",
            "DOF=10 testing\n",
            "Loaded data: X shape = (1000, 6), y shape = (1000, 10)\n",
            "DOF from data: 10\n",
            "Testing data: (1000, 6) -> (1000, 10)\n",
            "Evaluating KNN...\n",
            "  ✓ Joint RMSE: 1.9393, Inference: 0.014s\n",
            "Evaluating ELM...\n",
            "  ✓ Joint RMSE: 1.8119, Inference: 0.000s\n",
            "Evaluating RandomForest...\n",
            "  ✓ Joint RMSE: 1.8489, Inference: 0.013s\n",
            "Evaluating SVM...\n",
            "  ✓ Joint RMSE: 1.8621, Inference: 0.800s\n",
            "Evaluating ANN...\n",
            "  ✓ Joint RMSE: 1.7991, Inference: 0.001s\n",
            "Evaluating MDN...\n",
            "  ✓ Joint RMSE: 1.8080, Inference: 0.001s\n",
            "Evaluating CVAE...\n",
            "  ✓ Joint RMSE: 2.1309, Inference: 0.001s\n",
            "\n",
            "dof=10 testing complete\n",
            "\n",
            "testing complete\n",
            "============================================================\n",
            "Complete Results:\n",
            "    dof         model  joint_rmse  training_time  inference_time_per_sample\n",
            "0     3           KNN      1.0648         0.0181                     0.0000\n",
            "1     3           ELM      1.3555         0.2455                     0.0000\n",
            "2     3  RandomForest      0.8570         0.5002                     0.0000\n",
            "3     3           SVM      1.1026         0.7064                     0.0001\n",
            "4     3           ANN      1.3034         7.0416                     0.0000\n",
            "5     3           MDN      1.3428        14.9865                     0.0000\n",
            "6     3          CVAE      0.9618         9.8138                     0.0000\n",
            "7     4           KNN      1.4249         0.0080                     0.0000\n",
            "8     4           ELM      1.5366         0.0292                     0.0000\n",
            "9     4  RandomForest      1.3798         0.5930                     0.0000\n",
            "10    4           SVM      1.4019         0.9478                     0.0003\n",
            "11    4           ANN      1.4887         0.8179                     0.0000\n",
            "12    4           MDN      1.5188         9.8790                     0.0000\n",
            "13    4          CVAE      1.3593        10.8297                     0.0000\n",
            "14    5           KNN      1.6308         0.0092                     0.0000\n",
            "15    5           ELM      1.6597         0.0978                     0.0000\n",
            "16    5  RandomForest      1.5741         0.5510                     0.0000\n",
            "17    5           SVM      1.5898         1.2242                     0.0005\n",
            "18    5           ANN      1.6392         0.8801                     0.0000\n",
            "19    5           MDN      1.6246        10.0546                     0.0000\n",
            "20    5          CVAE      1.8070         9.9164                     0.0000\n",
            "21    6           KNN      1.8155         0.0080                     0.0000\n",
            "22    6           ELM      1.7616         0.0638                     0.0000\n",
            "23    6  RandomForest      1.7764         0.7061                     0.0000\n",
            "24    6           SVM      1.7739         1.5600                     0.0011\n",
            "25    6           ANN      1.7471         0.8374                     0.0000\n",
            "26    6           MDN      1.7507        10.5976                     0.0000\n",
            "27    6          CVAE      2.0519         9.8365                     0.0000\n",
            "28    7           KNN      1.9012         0.0077                     0.0000\n",
            "29    7           ELM      1.8015         0.0148                     0.0000\n",
            "30    7  RandomForest      1.8182         0.6023                     0.0000\n",
            "31    7           SVM      1.8296         2.2276                     0.0006\n",
            "32    7           ANN      1.7809         1.1424                     0.0000\n",
            "33    7           MDN      1.7861         9.6721                     0.0000\n",
            "34    7          CVAE      2.1149        10.0457                     0.0000\n",
            "35    8           KNN      1.9224         0.0078                     0.0000\n",
            "36    8           ELM      1.8035         0.0284                     0.0000\n",
            "37    8  RandomForest      1.8431         0.6552                     0.0000\n",
            "38    8           SVM      1.8458         1.7849                     0.0007\n",
            "39    8           ANN      1.7904         0.8414                     0.0000\n",
            "40    8           MDN      1.8047        10.0350                     0.0000\n",
            "41    8          CVAE      2.1735         9.9671                     0.0000\n",
            "42    9           KNN      1.9272         0.0347                     0.0000\n",
            "43    9           ELM      1.7976         0.0540                     0.0000\n",
            "44    9  RandomForest      1.8416         0.6922                     0.0000\n",
            "45    9           SVM      1.8559         1.9667                     0.0008\n",
            "46    9           ANN      1.7931         0.8770                     0.0000\n",
            "47    9           MDN      1.7973        10.1633                     0.0000\n",
            "48    9          CVAE      2.1482         9.3811                     0.0000\n",
            "49   10           KNN      1.9393         0.0063                     0.0000\n",
            "50   10           ELM      1.8119         0.0194                     0.0000\n",
            "51   10  RandomForest      1.8489         1.0061                     0.0000\n",
            "52   10           SVM      1.8621         3.0495                     0.0008\n",
            "53   10           ANN      1.7991         0.8446                     0.0000\n",
            "54   10           MDN      1.8080         9.7068                     0.0000\n",
            "55   10          CVAE      2.1309         9.6756                     0.0000\n",
            "\n",
            "Results saved to: /content/NNIK/results/explicit_training_results.csv\n",
            "\n",
            "Model Performance Summary (Average across DOFs):\n",
            "              joint_rmse  training_time  inference_time_per_sample\n",
            "model                                                             \n",
            "ANN               1.6677         1.6603                     0.0000\n",
            "CVAE              1.8434         9.9332                     0.0000\n",
            "ELM               1.6910         0.0691                     0.0000\n",
            "KNN               1.7033         0.0125                     0.0000\n",
            "MDN               1.6791        10.6369                     0.0000\n",
            "RandomForest      1.6174         0.6633                     0.0000\n",
            "SVM               1.6577         1.6834                     0.0006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization and Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from scipy.spatial import ConvexHull\n",
        "from scipy.interpolate import griddata\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup consistent visual style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "def colors(results_df):\n",
        "    # Create consistent color mapping for models\n",
        "    unique_models = results_df['model'].unique()\n",
        "    return dict(zip(unique_models, sns.color_palette('husl', len(unique_models))))\n",
        "\n",
        "def accuracy_vs_speed(results_df, ax, model_colors):\n",
        "    # Plot accuracy vs speed\n",
        "    for model in results_df['model'].unique():\n",
        "        model_data = results_df[results_df['model'] == model]\n",
        "        avg_acc = model_data['joint_rmse'].mean()\n",
        "        avg_time = model_data['training_time'].mean()\n",
        "        ax.scatter(avg_time, avg_acc, s=120, alpha=0.8,\n",
        "                  label=model, color=model_colors[model], edgecolors='white', linewidth=1)\n",
        "        ax.annotate(model, (avg_time, avg_acc), xytext=(8, 8),\n",
        "                   textcoords='offset points', fontsize=9, fontweight='bold')\n",
        "\n",
        "    ax.set_xlabel('Average Training Time (s)', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Average Joint RMSE', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Accuracy vs Speed Tradeoff', fontsize=12, fontweight='bold', pad=20)\n",
        "    ax.set_xscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "def heatmap(results_df, ax):\n",
        "    # Plot model performance heatmap with improved colormap\n",
        "    pivot_rmse = results_df.pivot_table(values='joint_rmse', index='model', columns='dof', aggfunc='mean')\n",
        "\n",
        "    # Use perceptually uniform colormap\n",
        "    im = ax.imshow(pivot_rmse.values, cmap='viridis', aspect='auto', interpolation='bilinear')\n",
        "    ax.set_xticks(range(len(pivot_rmse.columns)))\n",
        "    ax.set_xticklabels(pivot_rmse.columns, fontweight='bold')\n",
        "    ax.set_yticks(range(len(pivot_rmse.index)))\n",
        "    ax.set_yticklabels(pivot_rmse.index, fontweight='bold')\n",
        "    ax.set_xlabel('DOF', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Model', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Joint RMSE Heatmap', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "    # Add colorbar with proper positioning\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "    cbar.set_label('Joint RMSE', fontweight='bold')\n",
        "\n",
        "def training_distribution(results_df, ax):\n",
        "    # Plot training time distribution\n",
        "    training_times = results_df.groupby('model')['training_time'].mean().sort_values()\n",
        "    colors = ['green' if t < 10 else 'orange' if t < 30 else 'red' for t in training_times.values]\n",
        "\n",
        "    bars = ax.bar(range(len(training_times)), training_times.values, color=colors, alpha=0.8, edgecolor='white')\n",
        "    ax.set_xticks(range(len(training_times)))\n",
        "    ax.set_xticklabels(training_times.index, rotation=45, ha='right', fontweight='bold')\n",
        "    ax.set_ylabel('Training Time (s)', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Average Training Time by Model', fontsize=12, fontweight='bold', pad=20)\n",
        "    ax.set_yscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars, training_times.values):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
        "               f'{val:.1f}s', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "def speed_comparison(results_df, ax):\n",
        "    # Plot inference speed comparison\n",
        "    inference_times = results_df.groupby('model')['inference_time_per_sample'].mean() * 1000\n",
        "    inference_times = inference_times.sort_values()\n",
        "    colors = ['green' if t < 1 else 'orange' if t < 5 else 'red' for t in inference_times.values]\n",
        "\n",
        "    bars = ax.barh(range(len(inference_times)), inference_times.values, color=colors, alpha=0.8, edgecolor='white')\n",
        "    ax.set_yticks(range(len(inference_times)))\n",
        "    ax.set_yticklabels(inference_times.index, fontweight='bold')\n",
        "    ax.set_xlabel('Inference Time (ms/sample)', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Inference Speed Ranking', fontsize=12, fontweight='bold', pad=20)\n",
        "    ax.set_xscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "def model_distribution(results_df, ax):\n",
        "    # Plot pie chart of best model distribution\n",
        "    best_per_dof = results_df.loc[results_df.groupby('dof')['joint_rmse'].idxmin()]\n",
        "    dof_counts = best_per_dof['model'].value_counts()\n",
        "\n",
        "    colors = sns.color_palette('husl', len(dof_counts))\n",
        "    wedges, texts, autotexts = ax.pie(dof_counts.values, labels=dof_counts.index, autopct='%1.1f%%',\n",
        "                                     startangle=90, colors=colors, textprops={'fontweight': 'bold'})\n",
        "    ax.set_title('Best Model Distribution Across DOFs', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "def performance_improvement(results_df, ax):\n",
        "    # Plot performance improvement with increasing DOF\n",
        "    for model in results_df['model'].unique():\n",
        "        model_data = results_df[results_df['model'] == model].sort_values('dof')\n",
        "        if len(model_data) > 1:\n",
        "            improvement = (model_data['joint_rmse'].values[:-1] - model_data['joint_rmse'].values[1:]) / model_data['joint_rmse'].values[:-1] * 100\n",
        "            ax.plot(model_data['dof'].values[1:], improvement, 'o-', label=model, alpha=0.8, linewidth=2)\n",
        "\n",
        "    ax.set_xlabel('DOF', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Performance Change (%)', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Performance Change with Increasing DOF', fontsize=12, fontweight='bold', pad=20)\n",
        "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# NEW VISUALIZATION 1: 3D Accuracy-Speed-DOF Plot\n",
        "def 3d_plot(results_df, model_colors):\n",
        "    # 3D plot showing DOF vs Inference Speed vs Joint RMSE\n",
        "    fig = plt.figure(figsize=(12, 9))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    for model in results_df['model'].unique():\n",
        "        model_data = results_df[results_df['model'] == model]\n",
        "        x = model_data['dof']\n",
        "        y = model_data['inference_time_per_sample'] * 1000  # Convert to ms\n",
        "        z = model_data['joint_rmse']\n",
        "\n",
        "        ax.scatter(x, y, z, c=[model_colors[model]], s=80, alpha=0.8,\n",
        "                  label=model, edgecolors='white', linewidth=1)\n",
        "\n",
        "    ax.set_xlabel('DOF', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Inference Time (ms)', fontsize=11, fontweight='bold')\n",
        "    ax.set_zlabel('Joint RMSE', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('3D Model Performance Tradeoffs\\n(DOF vs Speed vs Accuracy)',\n",
        "                fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.legend(bbox_to_anchor=(1.15, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def pareto_frontier_analysis(results_df, model_colors):\n",
        "    # Create Pareto frontier analysis for accuracy vs inference speed\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Calculate average metrics per model\n",
        "    model_metrics = results_df.groupby('model').agg({\n",
        "        'joint_rmse': 'mean',\n",
        "        'inference_time_per_sample': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Convert to ms\n",
        "    model_metrics['inference_time_ms'] = model_metrics['inference_time_per_sample'] * 1000\n",
        "\n",
        "    # Plot all models\n",
        "    for _, row in model_metrics.iterrows():\n",
        "        ax.scatter(row['inference_time_ms'], row['joint_rmse'],\n",
        "                  s=150, alpha=0.8, color=model_colors[row['model']],\n",
        "                  edgecolors='white', linewidth=2, label=row['model'])\n",
        "        ax.annotate(row['model'], (row['inference_time_ms'], row['joint_rmse']),\n",
        "                   xytext=(8, 8), textcoords='offset points',\n",
        "                   fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Find Pareto frontier (minimize both inference time and RMSE)\n",
        "    points = model_metrics[['inference_time_ms', 'joint_rmse']].values\n",
        "    pareto_front = []\n",
        "\n",
        "    for i, point in enumerate(points):\n",
        "        dominated = False\n",
        "        for other_point in points:\n",
        "            if (other_point[0] <= point[0] and other_point[1] <= point[1] and\n",
        "                (other_point[0] < point[0] or other_point[1] < point[1])):\n",
        "                dominated = True\n",
        "                break\n",
        "        if not dominated:\n",
        "            pareto_front.append(i)\n",
        "\n",
        "    # Plot Pareto frontier\n",
        "    if len(pareto_front) > 1:\n",
        "        pareto_points = points[pareto_front]\n",
        "        sorted_indices = np.argsort(pareto_points[:, 0])\n",
        "        pareto_sorted = pareto_points[sorted_indices]\n",
        "        ax.plot(pareto_sorted[:, 0], pareto_sorted[:, 1], 'r--',\n",
        "               linewidth=3, alpha=0.8, label='Pareto Frontier')\n",
        "\n",
        "    ax.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Joint RMSE (lower is better)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Pareto Frontier Analysis: Accuracy vs Speed Tradeoff',\n",
        "                fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_xscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def variance_analysis(results_df, model_colors):\n",
        "    # Create error bar plots showing model stability across DOF\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Joint RMSE variance across DOF\n",
        "    for model in results_df['model'].unique():\n",
        "        model_data = results_df[results_df['model'] == model]\n",
        "        dof_stats = model_data.groupby('dof')['joint_rmse'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "        ax1.errorbar(dof_stats['dof'], dof_stats['mean'], yerr=dof_stats['std'],\n",
        "                    marker='o', linewidth=2, markersize=6, alpha=0.8,\n",
        "                    color=model_colors[model], label=model, capsize=5)\n",
        "\n",
        "    ax1.set_xlabel('DOF', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Joint RMSE', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Model Stability: RMSE vs DOF', fontsize=12, fontweight='bold', pad=20)\n",
        "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Training time variance across DOF\n",
        "    for model in results_df['model'].unique():\n",
        "        model_data = results_df[results_df['model'] == model]\n",
        "        dof_stats = model_data.groupby('dof')['training_time'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "        ax2.errorbar(dof_stats['dof'], dof_stats['mean'], yerr=dof_stats['std'],\n",
        "                    marker='s', linewidth=2, markersize=6, alpha=0.8,\n",
        "                    color=model_colors[model], label=model, capsize=5)\n",
        "\n",
        "    ax2.set_xlabel('DOF', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Training Time (s)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Model Stability: Training Time vs DOF', fontsize=12, fontweight='bold', pad=20)\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def heatmap(results_df):\n",
        "    # Create high-resolution heatmap with smooth interpolation\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Prepare data\n",
        "    models = results_df['model'].unique()\n",
        "    dofs = sorted(results_df['dof'].unique())\n",
        "\n",
        "    # Create grid for interpolation\n",
        "    model_indices = {model: i for i, model in enumerate(models)}\n",
        "    results_df['model_idx'] = results_df['model'].map(model_indices)\n",
        "\n",
        "    # Joint RMSE heatmap with interpolation\n",
        "    pivot_rmse = results_df.pivot_table(values='joint_rmse', index='model', columns='dof', aggfunc='mean')\n",
        "\n",
        "    # Create smooth interpolation\n",
        "    xi = np.linspace(0, len(models)-1, len(models)*3)\n",
        "    yi = np.linspace(min(dofs), max(dofs), len(dofs)*3)\n",
        "    xi_grid, yi_grid = np.meshgrid(xi, yi)\n",
        "\n",
        "    # Interpolate\n",
        "    points = []\n",
        "    values = []\n",
        "    for model in models:\n",
        "        for dof in dofs:\n",
        "            if not pd.isna(pivot_rmse.loc[model, dof]):\n",
        "                points.append([model_indices[model], dof])\n",
        "                values.append(pivot_rmse.loc[model, dof])\n",
        "\n",
        "    if len(points) > 3:  # Need at least 3 points for interpolation\n",
        "        zi = griddata(points, values, (xi_grid, yi_grid), method='cubic', fill_value=np.nan)\n",
        "        im1 = ax1.imshow(zi, extent=[0, len(models)-1, min(dofs), max(dofs)],\n",
        "                        aspect='auto', origin='lower', cmap='plasma', interpolation='bilinear')\n",
        "    else:\n",
        "        im1 = ax1.imshow(pivot_rmse.values, cmap='plasma', aspect='auto')\n",
        "\n",
        "    ax1.set_xticks(range(len(models)))\n",
        "    ax1.set_xticklabels(models, rotation=45, ha='right', fontweight='bold')\n",
        "    ax1.set_ylabel('DOF', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Smooth Joint RMSE Heatmap', fontsize=12, fontweight='bold', pad=20)\n",
        "    cbar1 = plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
        "    cbar1.set_label('Joint RMSE', fontweight='bold')\n",
        "\n",
        "    # Training time heatmap\n",
        "    pivot_time = results_df.pivot_table(values='training_time', index='model', columns='dof', aggfunc='mean')\n",
        "    im2 = ax2.imshow(pivot_time.values, cmap='magma', aspect='auto')\n",
        "    ax2.set_xticks(range(len(pivot_time.columns)))\n",
        "    ax2.set_xticklabels(pivot_time.columns, fontweight='bold')\n",
        "    ax2.set_yticks(range(len(pivot_time.index)))\n",
        "    ax2.set_yticklabels(pivot_time.index, fontweight='bold')\n",
        "    ax2.set_xlabel('DOF', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Training Time Heatmap', fontsize=12, fontweight='bold', pad=20)\n",
        "    cbar2 = plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
        "    cbar2.set_label('Training Time (s)', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def correlation_analysis(results_df):\n",
        "    # Create correlation heatmap of all metrics\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Select numeric columns for correlation\n",
        "    numeric_cols = ['dof', 'joint_rmse', 'training_time', 'inference_time_per_sample']\n",
        "    corr_data = results_df[numeric_cols].copy()\n",
        "    corr_data['inference_time_ms'] = corr_data['inference_time_per_sample'] * 1000\n",
        "    corr_data = corr_data.drop('inference_time_per_sample', axis=1)\n",
        "\n",
        "    # Overall correlation matrix\n",
        "    corr_matrix = corr_data.corr()\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "                square=True, ax=ax1, cbar_kws={'shrink': 0.8})\n",
        "    ax1.set_title('Overall Metric Correlations', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "    # Per-model correlation strength\n",
        "    model_correlations = []\n",
        "    for model in results_df['model'].unique():\n",
        "        model_data = results_df[results_df['model'] == model][numeric_cols]\n",
        "        if len(model_data) > 3:  # Need sufficient data points\n",
        "            model_corr = abs(model_data.corr()).mean().mean()  # Average absolute correlation\n",
        "            model_correlations.append({'model': model, 'avg_correlation': model_corr})\n",
        "\n",
        "    if model_correlations:\n",
        "        corr_df = pd.DataFrame(model_correlations)\n",
        "        bars = ax2.bar(corr_df['model'], corr_df['avg_correlation'],\n",
        "                      color=sns.color_palette('viridis', len(corr_df)), alpha=0.8)\n",
        "        ax2.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "        ax2.set_ylabel('Average Absolute Correlation', fontsize=12, fontweight='bold')\n",
        "        ax2.set_title('Model Metric Interdependence', fontsize=12, fontweight='bold', pad=20)\n",
        "        ax2.set_xticklabels(corr_df['model'], rotation=45, ha='right', fontweight='bold')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# MAIN ANALYSIS EXECUTION\n",
        "if all_test_results:\n",
        "    print('Enhanced Analytics Dashboard')\n",
        "\n",
        "    results_df = pd.DataFrame(all_test_results)\n",
        "    model_colors = colors(results_df)\n",
        "\n",
        "    # 1. ORIGINAL PLOTS (IMPROVED)\n",
        "    fig_main = plt.figure(figsize=(20, 12))\n",
        "    gs = fig_main.add_gridspec(2, 3, hspace=0.35, wspace=0.35)\n",
        "    fig_main.suptitle('Model Performance Analysis Dashboard', fontsize=16, fontweight='bold', y=0.95)\n",
        "\n",
        "    # Original 6 plots with improvements\n",
        "    ax1 = fig_main.add_subplot(gs[0, 0])\n",
        "    plot_accuracy_vs_speed_tradeoff(results_df, ax1, model_colors)\n",
        "\n",
        "    ax2 = fig_main.add_subplot(gs[0, 1])\n",
        "    heatmap(results_df, ax2)\n",
        "\n",
        "    ax3 = fig_main.add_subplot(gs[0, 2])\n",
        "    training_distribution(results_df, ax3)\n",
        "\n",
        "    ax4 = fig_main.add_subplot(gs[1, 0])\n",
        "    speed_comparison(results_df, ax4)\n",
        "\n",
        "    ax5 = fig_main.add_subplot(gs[1, 1])\n",
        "    model_distribution(results_df, ax5)\n",
        "\n",
        "    ax6 = fig_main.add_subplot(gs[1, 2])\n",
        "    performance_improvement(results_df, ax6)\n",
        "\n",
        "    plt.savefig(results_path / 'comprehensive_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. NEW ADVANCED VISUALIZATIONS\n",
        "\n",
        "    # 3D Tradeoff Analysis\n",
        "    fig_3d = 3d_plot(results_df, model_colors)\n",
        "    fig_3d.savefig(results_path / '3d_tradeoff_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Pareto Frontier Analysis\n",
        "    fig_pareto = pareto_frontier_analysis(results_df, model_colors)\n",
        "    fig_pareto.savefig(results_path / 'pareto_frontier_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Variance Analysis\n",
        "    fig_variance = variance_analysis(results_df, model_colors)\n",
        "    fig_variance.savefig(results_path / 'variance_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Granular Heatmap\n",
        "    fig_heatmap = heatmap(results_df)\n",
        "    fig_heatmap.savefig(results_path / 'granular_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation Analysis\n",
        "    fig_corr = correlation_analysis(results_df)\n",
        "    fig_corr.savefig(results_path / 'correlation_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(f'\\n')\n",
        "    print('ENHANCED PERFORMANCE STATISTICS')\n",
        "\n",
        "    print(f'Total experiments: {len(results_df)}')\n",
        "    print(f'Successful models: {len(results_df[results_df['status'] == 'success'])}')\n",
        "    print(f'Average accuracy (Joint RMSE): {results_df['joint_rmse'].mean():.4f}')\n",
        "    print(f'Best overall accuracy: {results_df['joint_rmse'].min():.4f}')\n",
        "    print(f'Fastest training: {results_df['training_time'].min():.2f}s')\n",
        "    print(f'Fastest inference: {results_df['inference_time_per_sample'].min()*1000:.2f}ms')\n",
        "\n",
        "    best_accuracy_model = results_df.loc[results_df['joint_rmse'].idxmin(), 'model']\n",
        "    fastest_training_model = results_df.loc[results_df['training_time'].idxmin(), 'model']\n",
        "    fastest_inference_model = results_df.loc[results_df['inference_time_per_sample'].idxmin(), 'model']\n",
        "\n",
        "    print(f'\\nInsights:')\n",
        "    print(f'Most accurate model: {best_accuracy_model}')\n",
        "    print(f'Fastest training: {fastest_training_model}')\n",
        "    print(f'Fastest inference: {fastest_inference_model}')\n",
        "\n",
        "    results_df['balanced_score'] = (\n",
        "        (1 / results_df['joint_rmse']) * 0.4 +\n",
        "        (1 / results_df['training_time']) * 0.3 +\n",
        "        (1 / results_df['inference_time_per_sample']) * 0.3\n",
        "    )\n",
        "\n",
        "    best_balanced = results_df.loc[results_df['balanced_score'].idxmax()]\n",
        "    print(f'Best balanced model: {best_balanced['model']} (DOF: {best_balanced['dof']})')\n",
        "\n",
        "    print(f'\\nEnhanced analysis complete!')\n",
        "    print(f'All results and visualizations saved to: {results_path}')\n",
        "\n",
        "else:\n",
        "    print('No results data available for analytics')"
      ],
      "metadata": {
        "id": "XspIBwkZvlaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}